{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface\n",
    "\n",
    "## Get started\n",
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install huggingface_hub\n",
    "# `huggingface-cli login` with hf write access token\n",
    "%pip install transformers\n",
    "%pip install jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, InferenceClient, AsyncInferenceClient\n",
    "from transformers import AutoTokenizer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage a repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = HfApi()\n",
    "api.create_repo(\"biaotang/test-dataset\", repo_type=\"dataset\", private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = api.list_repo_refs(\"biaotang/test-dataset\", repo_type=\"dataset\")\n",
    "pprint(refs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What is the capital of France?\",\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "output = client.chat_completion(messages, max_tokens=100)\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemma = InferenceClient(\"google/gemma-3-27b-it\")\n",
    "gemma.chat_completion(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_client = AsyncInferenceClient()\n",
    "async for token in await async_client.text_generation(\"The Huggingface Hub is\", stream=True):\n",
    "  print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_inference_endpoint\n",
    "# create_inference_endpoint(...), or\n",
    "# https://ui.endpoints.huggingface.co/new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messages to prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI assistant with access to various tools.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi !\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi human, what can help you with ?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")\n",
    "rendered_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "pprint(rendered_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent\n",
    "### Smolagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install 'smolagents[transformers,litellm]'\n",
    "%pip install -U 'google-cloud-aiplatform>=1.38'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# run: llama-server -m model.gguf --port 8080\n",
    "client = InferenceClient(model=\"http://localhost:8080/v1/chat/completions\")\n",
    "\n",
    "def custom_model(messages, stop_sequences=[\"Task\"]):\n",
    "    response = client.chat_completion(messages, stop=stop_sequences, max_tokens=1000)\n",
    "    answer = response.choices[0].message\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import TransformersModel\n",
    "\n",
    "model = TransformersModel(model_id=\"HuggingFaceTB/SmolLM-135M-Instruct\")\n",
    "\n",
    "print(model([{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Ok!\"}]}], stop_sequences=[\"great\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import HfApiModel\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Hello, how are you?\"}]}\n",
    "]\n",
    "\n",
    "model = HfApiModel()\n",
    "print(model(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "# Config gemini api key and vertex credential.\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import LiteLLMModel\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"what is the capital of France?\"}]}\n",
    "]\n",
    "# Gemini api access with api key, see .env file\n",
    "model = LiteLLMModel(model_id=\"gemini/gemini-2.0-flash\",\n",
    "                     temperature=0.2, max_tokens=10,)\n",
    "print(model(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import LiteLLMModel\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"what is the capital of France?\"}]}\n",
    "]\n",
    "# Vertex api access with application default credentail (ADC)\n",
    "model = LiteLLMModel(model_id=\"vertex_ai/gemini-2.0-flash\",\n",
    "                     temperature=0.2, max_tokens=10,)\n",
    "print(model(messages))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
